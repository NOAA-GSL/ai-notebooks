{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Running AI Weather Prediction (AIWP) models\n",
        "\n",
        "This notebook will guide you through the process of running AIWP models yourself using only free cloud resources provided through Google Colab. if you aren't interested in running the models yourself but would like to see output, use our other notebook.\n",
        "\n",
        "**Points where user interaction is required are marked in bold**\n",
        "\n",
        "Estimated Time: 70 minutes\n",
        "\n",
        "Questions? Contact jacob.radford@noaa.gov"
      ],
      "metadata": {
        "id": "x52s30_ypJ-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) AI Weather Prediction Background Information"
      ],
      "metadata": {
        "id": "GwjJdGNgf61D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1) What is an AI Weather Prediction Model?\n",
        "\n",
        "AIWP models are weather models that have been trained to identify relationships between the state of the atmosphere at time t=0 and the state of the atmosphere at time t+1 using historical data like ECMWF's Reanalysis v5 (ERA5). The model can then be combined with any initial atmospheric state to predict a future state. For example, the model can use the atmospheric state at 00 UTC on Jan 1, 2025 to make a weather prediction for 06 UTC on Jan 1, 2025. It then takes its prediction for 06 UTC on Jan 1, 2025 as the input to predict the state at 12 UTC on Jan 1, 2025, and so on until the desired forecast length is reached (usually 10 days). This iterative process is referred to as an autoregressive model."
      ],
      "metadata": {
        "id": "YpqRZFjnoMd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2) How do they differ from traditional weather models (AKA Numerical Weather Prediction)?\n",
        "\n",
        "Numerical Weather Prediction models use partial differential equations that represent our physical understanding of the atmosphere - things like conservation of momentum, conservation of energy, etc. AIWP models don't need any of this information because, in a way, they can learn these sorts of relationships from the historical data. NWP models are very computationally expensive and take a long time to run, even on supercomputers. Meanwhile, you can run an AIWP model in minutes right here!"
      ],
      "metadata": {
        "id": "stp0BnY3oQXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3) What about forecast skill? How do AIWP models compare to NWP models?\n",
        "\n",
        "State of the art AIWP models have been shown to have performance comparable to or better than NWP models in many regards. For example, in this chart from WeatherBench2, blue squares indicate variables where the AIWP model outperforms the IFS HRES NWP model (a state of the art NWP model) in terms of root mean squared error (RMSE). The Pangu-Weather and GraphCast squares are almost all blue!\n",
        "\n",
        "![WeatherBench2](https://shorturl.at/AUtBV)\n",
        "\n",
        "Results have been very exciting for the meteorological community. That said, there are some caveats - perhaps the the biggest one is that which model is \"better\" can be subjective. Just because one model has lower RMSE than another doesn't mean it's necessarily better for your use case. For example, a couple of commonly cited limitations for AIWP models is that they may struggle to predict extreme events like strong hurricanes or intense precipitation and that they tend to produce \"blurry\" looking output for longer range forecasts. Furthermore, one model may perform better than another in particular regions or for specific types of weather events. More research is being done to identify specific strengths and weaknesses of different models."
      ],
      "metadata": {
        "id": "LexaVB3ZoVLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4) If I can run competitive weather models myself, will that make NOAA obsolete?\n",
        "\n",
        "Definitely not. There are many reasons that NOAA will have a vital role in the future of weather forecasting, AI-based or otherwise:\n",
        "\n",
        "1.   AIWP models still fundamentally rely on NOAA's data assimilation processes. Data assimilation is the process through which weather observations across the globe are combined to create a \"best-guess\" at the current state of the atmosphere. The output of the data assimilation system is what is then used to initialize AIWP models.\n",
        "2.   Most AIWP models are limited in spatial resolution to 0.25° (~30km) because they are trained on ERA5 data, which itself has a resolution of 0.25°. 0.25° models are unable to capture many smaller, impactful meteorological features like thunderstorms. Traditional NWP models like the High-Resolution Rapid Refresh have spatial resolution of 3km and are much more capable of resolving smaller scale features.\n",
        "3.   AIWP models predict a limited subset of meteorological variables. For example, variables related to precipitation type, wind gusts, and cloud parameters are currently not predicted by any AIWP models.\n",
        "4.   There are still lingering questions about how well AIWP models can represent extreme weather events like hurricanes, intense precipitation, cold snaps, heat waves, etc.\n",
        "5.   NOAA is taking an active role in AIWP model research and exploring how to incorporate AIWP into operations to make weather forecasts more accurate.\n",
        "6.   The human factor: more accurate models are only one part of the equation. How the output of these models is then communicated to the general public is where NOAA really shines."
      ],
      "metadata": {
        "id": "T2i3XDJ0oW0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Getting Started with AIWP on Google Colab\n",
        "\n",
        "In this section we give instructions on connecting to Google Colab cloud resources, configuring the environment, running an AIWP model, and converting the output from grib format to NetCDF4 format.\n",
        "\n",
        "**Section 2 Estimated Time: 20 minutes**"
      ],
      "metadata": {
        "id": "z6hSZEkUp2gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1) Connecting to a runtime\n",
        "\n",
        "In the upper right corner of this page you will see \"Connect\" with a dropdown arrow next to it. This is how you will connect to the cloud compute resources needed to run AIWP models. Which type of runtime you need to connect to depends on which models you want to run and whether or not you have paid for Colab compute units.\n",
        "\n",
        "Not all models can run on all runtime types. See the table below to check compatability and speed. Some models, like 0.25° GraphCast, can't be run on any of the Colab resources (at least in my trials)!\n",
        "\n",
        "Color code:\n",
        "\n",
        "*   Green - Model runs fast on runtime\n",
        "*   Yellow - Model can technically run on runtime, but slowly\n",
        "*   Red - Model cannot be run on runtime\n",
        "\n",
        "![Benchmarks](https://i.imgur.com/uvRmq4L.png)\n",
        "\n",
        "**For the purposes of this tutorial, we will use the free T4 GPU, which can run FourCastNetv2-small, Pangu-Weather, and GraphCast 1.00°.**\n",
        "\n",
        "**Click the dropdown arrow, then \"change runtime type,\" select T4 GPU and save. Finally, click \"connect\" to connect to the runtime.**\n",
        "\n",
        "**T4 GPUs are typically, but not always, available.**"
      ],
      "metadata": {
        "id": "kRyrf-eYfeKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2) Configuring the environment (what's under the hood?)\n",
        "\n",
        "The next step for running AIWP models is installing packages built specifically for this purpose.\n",
        "\n",
        "The core functionality comes from a package developed by ECMWF called [ai-models](https://github.com/ecmwf-lab/ai-models). ai-models does everything for you - retrieving the data, formatting the data to what is expected by each AIWP model, running the models, and writing the output to grib files.\n",
        "\n",
        "ai-models is limited to ECMWF hosted data like ERA5 and IFS initial conditions, which require API keys and doesn't include NOAA data. To address this, I made a branch of ai-models called ai-models-gfs that can also retrieve GFS, GDAS, or GEFS initial conditions.\n",
        "\n",
        "The other packages (like ai-models-graphcast-gfs) are just plug-ins for the ai-models package to allow it to run each model.\n",
        "\n",
        "After that we import all of the packages needed for the whole notebook.\n",
        "\n",
        "**Run this cell wihout modification.**\n",
        "\n",
        "**At some point Colab may tell you that you need to restart the runtime. If it does, restart and then run this cell again.**\n"
      ],
      "metadata": {
        "id": "drfMtdvVa4QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install all of the required packages\n",
        "!pip install ai-models-gfs\n",
        "!pip install ai-models-graphcast-gfs\n",
        "!pip install ai-models-fourcastnetv2-gfs\n",
        "!pip install ai-models-panguweather-gfs\n",
        "!pip install ai-models-aurora-gfs\n",
        "!pip install git+https://github.com/deepmind/graphcast.git\n",
        "!pip install xarray pygrib awscli\n",
        "!pip install basemap basemap-data-hires\n",
        "\n",
        "#!pip install jax[tpu] // Uncomment if running models on a TPU runtime\n",
        "\n",
        "#These are the imports needed for the rest of the notebook\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import pygrib as pg\n",
        "import ipywidgets as widgets\n",
        "import math\n",
        "import datetime\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from mpl_toolkits.basemap import Basemap, shiftgrid\n",
        "from IPython.display import HTML\n",
        "from typing import Optional\n",
        "from netCDF4 import Dataset as DS"
      ],
      "metadata": {
        "id": "ZN8q63280R4R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3) Downloading model assets\n",
        "\n",
        "Before running the models we need to download them! This can technically be handled internally by ai-models or ai-models-gfs, but it's a bit faster to just download them before-hand from our S3 bucket with AWS command line interface.\n",
        "\n",
        "**Run this cell without modification.**"
      ],
      "metadata": {
        "id": "Vx1opLAk0YrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive --no-sign-request s3://noaa-oar-mlwp-data/colab_resources/fcnv2 ./fcnv2/\n",
        "!aws s3 cp --recursive --no-sign-request s3://noaa-oar-mlwp-data/colab_resources/pw ./pw/\n",
        "!aws s3 cp --recursive --no-sign-request s3://noaa-oar-mlwp-data/colab_resources/au ./au/\n",
        "!aws s3 cp --recursive --no-sign-request s3://noaa-oar-mlwp-data/colab_resources/gc ./gc/"
      ],
      "metadata": {
        "id": "jzRQsh_g0YV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.4) Running our first model\n",
        "\n",
        "Now we're all set to run our first AIWP model!\n",
        "\n",
        "We'll start with FourCastNetv2-small because it's fast and chronologically came first out of the models available here.\n",
        "\n",
        "The first time you run this the model weights will need to be downloaded which can take a little while, after which the model will run.\n",
        "\n",
        "**Specify the date, time, and model. You can also change the output path and how far out (lead time) the model runs.**\n",
        "\n",
        "**For the first time through the notebook, I recommend keeping the default date/time/model/initial condition configuration.**"
      ],
      "metadata": {
        "id": "sXNN9xq4fQ6X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwW3w6Ag0PWx"
      },
      "outputs": [],
      "source": [
        "# Define input parameters\n",
        "\n",
        "# Any date after Jan 1, 2021 in YYYYmmdd format\n",
        "date = \"20240702\"\n",
        "\n",
        "# Initialization time - must be one of [\"0000\", \"0600\", \"1200\", \"1800\"]\n",
        "time = \"1200\"\n",
        "\n",
        "# Model choices:\n",
        "# [\"fourcastnetv2-small\", \"panguweather\", \"graphcast\",\n",
        "# \"graphcast-1p00\", \"aurora-2.5-finetuned\"]\n",
        "model = \"fourcastnetv2-small\"\n",
        "\n",
        "# Dataset to use for initial conditions:\n",
        "# \"gfs\" or \"gdas\" are easiest\n",
        "# An empty string defaults to ECMWF MARS (need an account and api key)\n",
        "# \"cds\" uses ERA5 (need an account and api key)\n",
        "input_data = \"gfs\"\n",
        "\n",
        "# Path to save output\n",
        "output_path = f\"./{model}_{date}_{time}\"\n",
        "# How far out to run model in hours\n",
        "forecast_lead = \"240\"\n",
        "\n",
        "\n",
        "# ** Don't change anything beyond this point **\n",
        "\n",
        "\n",
        "# Map model choice to assets path\n",
        "model2assets = {\n",
        "    \"fourcastnetv2-small\":\"./fcnv2\",\n",
        "    \"panguweather\":\"./pw\",\n",
        "    \"aurora-2.5-finetuned\":\"./au\",\n",
        "    \"graphcast\":\"./gc\",\n",
        "    \"graphcast-1p00\":\"./gc\"\n",
        "}\n",
        "\n",
        "assets = model2assets[model]\n",
        "\n",
        "# Runs the ai-models-gfs command\n",
        "!ai-models-gfs \\\n",
        "    --input {input_data} \\\n",
        "    --date {date} \\\n",
        "    --time {time} \\\n",
        "    --assets {assets} \\\n",
        "    --path {output_path} \\\n",
        "    --lead {forecast_lead} \\\n",
        "    {model}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.5) Convert to NetCDF file\n",
        "\n",
        "That's it! We ran a 240 hr weather forecast in just minutes. If you run that again (while still connected to the same runtime) for a different date or time it will be even faster because you won't have to download the models weights again.\n",
        "\n",
        "The next step of converting the output to a NetCDF isn't strictly necessary, but makes the data easier to analyze, in my opinion.\n",
        "\n",
        "**Run this cell without modification.**"
      ],
      "metadata": {
        "id": "6YxZCQHDygzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ** Don't modify any of this **\n",
        "\n",
        "# Define the main function to convert GRIB files to NetCDF\n",
        "def grib2nc(model, path, lead, step, date, time, inputmodel):\n",
        "\n",
        "    # Mapping of variable names and their descriptions/units\n",
        "    varmap = {\n",
        "        \"u10\": ['10 metre U wind component', 'm s-1'],\n",
        "        \"v10\": ['10 metre V wind component', 'm s-1'],\n",
        "        \"t2\": ['2 metre temperature', 'K'],\n",
        "        \"msl\": ['Pressure reduced to MSL', 'Pa'],\n",
        "        \"u100\": ['100 metre U wind component', 'm s-1'],\n",
        "        \"v100\": ['100 metre V wind component', 'm s-1'],\n",
        "        \"sp\": ['Surface pressure', 'Pa'],\n",
        "        \"tcwv\": ['Precipitable water', 'kg m-2'],\n",
        "        \"apcp\": ['6-hr accumulated precipitation', 'm'],\n",
        "        \"u\": ['U component of wind', 'm s-1'],\n",
        "        \"v\": ['V component of wind', 'm s-1'],\n",
        "        \"t\": ['Temperature', 'K'],\n",
        "        \"z\": ['Geopotential', 'm2 s-2'],\n",
        "        \"r\": ['Relative humidity', '%'],\n",
        "        \"q\": ['Specific humidity', 'kg kg-1'],\n",
        "        \"w\": ['Vertical velocity', 'Pa s-1']\n",
        "    }\n",
        "\n",
        "    # Mapping from ECMWF to GFS names\n",
        "    ec2gfsmap = {\n",
        "        \"u\": \"u\",\n",
        "        \"v\": \"v\",\n",
        "        \"w\": \"w\",\n",
        "        \"z\": \"z\",\n",
        "        \"q\": \"q\",\n",
        "        \"r\": \"r\",\n",
        "        \"t\": \"t\",\n",
        "        \"10u\": \"u10\",\n",
        "        \"10v\": \"v10\",\n",
        "        \"100u\": \"u100\",\n",
        "        \"100v\": \"v100\",\n",
        "        \"2t\": \"t2\",\n",
        "        \"msl\": \"msl\",\n",
        "        \"sp\": \"sp\",\n",
        "        \"tcwv\": \"tcwv\",\n",
        "        \"tp\": \"apcp\"\n",
        "    }\n",
        "\n",
        "    # Initialize lists to store variables and levels\n",
        "    unique_pl_vars = []\n",
        "    unique_sfc_vars = []\n",
        "    levels = []\n",
        "\n",
        "    # Open the GRIB file and extract grid shape and lat/lon coordinates\n",
        "    with pg.open(path) as grib:\n",
        "        y_shape, x_shape = grib[1].values.shape\n",
        "        lats, lons = grib[1].latlons()\n",
        "        lats = lats[:, 0]\n",
        "        lons = lons[0, :]\n",
        "\n",
        "        # Identify pressure level and surface variables\n",
        "        for grb in grib:\n",
        "            if grb.levelType == \"pl\":\n",
        "                if grb.level not in levels:\n",
        "                    levels.append(grb.level)\n",
        "                if grb.shortName not in unique_pl_vars:\n",
        "                    unique_pl_vars.append(grb.shortName)\n",
        "            else:\n",
        "                if grb.shortName not in unique_sfc_vars:\n",
        "                    unique_sfc_vars.append(grb.shortName)\n",
        "        levels.sort(reverse=True)\n",
        "        levelmap = {level: c for c, level in enumerate(levels)}\n",
        "\n",
        "        # Create NetCDF file and define dimensions\n",
        "        f = DS(f\"{path}.nc\", 'w', format='NETCDF4')\n",
        "        f.createDimension('time', lead // step + 1)\n",
        "        f.createDimension('level', len(levels))\n",
        "        f.createDimension('longitude', x_shape)\n",
        "        f.createDimension('latitude', y_shape)\n",
        "\n",
        "        # Generate time values based on initialization date and time\n",
        "        initdt = datetime.datetime.strptime(f\"{date}{time}\", \"%Y%m%d%H00\")\n",
        "        times = np.array([\n",
        "            int((initdt + datetime.timedelta(hours=int(i))).timestamp())\n",
        "            for i in np.arange(0, lead + step, step)\n",
        "        ])\n",
        "\n",
        "        # Create coordinate variables in NetCDF\n",
        "        create_variable_nochunk(f, 'time', ('time',), times, {\n",
        "            'long_name': 'Date and Time', 'units': 'seconds since 1970-1-1',\n",
        "            'calendar': 'standard'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'longitude', ('longitude',), lons, {\n",
        "            'long_name': 'Longitude', 'units': 'degree'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'latitude', ('latitude',), lats, {\n",
        "            'long_name': 'Latitude', 'units': 'degree'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'level', ('level',), np.array(levels), {\n",
        "            'long_name': 'Isobaric surfaces', 'units': 'hPa'\n",
        "        })\n",
        "\n",
        "        # Define variables in NetCDF for both pressure level and surface variables\n",
        "        for variable in unique_pl_vars + unique_sfc_vars:\n",
        "            dims = ('time', 'level', 'latitude', 'longitude') if variable in unique_pl_vars else ('time', 'latitude', 'longitude')\n",
        "            chunksizes = (1, 1, y_shape, x_shape) if 'level' in dims else (1, y_shape, x_shape)\n",
        "            gfsequivalent = ec2gfsmap[variable]\n",
        "            create_variable(\n",
        "                f, gfsequivalent, dims, None, {\n",
        "                    'long_name': varmap[gfsequivalent][0],\n",
        "                    'units': varmap[gfsequivalent][1]\n",
        "                },\n",
        "                chunksizes\n",
        "            )\n",
        "\n",
        "        # Populate NetCDF variables with data from GRIB\n",
        "        grib.seek(0)\n",
        "        for grb in grib:\n",
        "            shortName = grb.shortName\n",
        "            timestep = int(grb.step / step)\n",
        "            level = grb.level\n",
        "            levelType = grb.levelType\n",
        "            if (shortName == 'z' and levelType == 'sfc') or shortName not in ec2gfsmap.keys():\n",
        "                continue\n",
        "            gfsequivalent = ec2gfsmap[shortName]\n",
        "            vals = grb.values\n",
        "\n",
        "            if levelType == 'pl':\n",
        "                levelind = levelmap[level]\n",
        "                f.variables[gfsequivalent][timestep, levelind, :, :] = vals\n",
        "            elif levelType == 'sfc':\n",
        "                f.variables[gfsequivalent][timestep, :, :] = vals\n",
        "\n",
        "        # Add global attributes to the NetCDF file\n",
        "        f.Conventions = 'CF-1.8'\n",
        "        f.version = '3_2025-02-20'\n",
        "        f.model_name = model\n",
        "        f.initialization_model = inputmodel\n",
        "        f.initialization_time = initdt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        f.first_forecast_hour = \"0\"\n",
        "        f.last_forecast_hour = f\"{lead}\"\n",
        "        f.forecast_hour_step = f\"6\"\n",
        "        f.creation_time = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        f.close()\n",
        "\n",
        "# Functions to create variables in the NetCDF file\n",
        "def create_variable(f, name, dimensions, data, attrs, chunksizes):\n",
        "    dtype = 'i4' if name in ['time', 'level'] else 'f4'\n",
        "    var = f.createVariable(name, dtype, dimensions, compression='zlib', complevel=2, chunksizes=chunksizes)\n",
        "    if data is not None:\n",
        "        var[:] = data\n",
        "    for attr_name, attr_value in attrs.items():\n",
        "        var.setncattr(attr_name, attr_value)\n",
        "\n",
        "def create_variable_nochunk(f, name, dimensions, data, attrs):\n",
        "    dtype = 'i4' if name in ['time', 'level'] else 'f4'\n",
        "    var = f.createVariable(name, dtype, dimensions, compression='zlib', complevel=2)\n",
        "    var[:] = data\n",
        "    for attr_name, attr_value in attrs.items():\n",
        "        var.setncattr(attr_name, attr_value)\n",
        "\n",
        "#Call the function\n",
        "grib2nc(model, output_path, int(forecast_lead), 6, date, time, input_data)\n",
        "!rm {output_path}\n"
      ],
      "metadata": {
        "id": "y1bc08pomiGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Visualization\n",
        "\n",
        "In this section we provide two ways to visualize the output. First we make an animation - it provides functionality like play, pause, next, and previous buttons. After that we'll provide code for more traditional visualization with matplotlib.\n",
        "\n",
        "**Section 3 Estimated Time: 10 minutes**"
      ],
      "metadata": {
        "id": "pzy47_3s7aYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1) Define the variable information\n",
        "\n",
        "This cell is just a large dictionary defining some appropriate bounds for each variable and other general information like name and units.\n",
        "\n",
        "**Only change this dictionary if you want to update minimum and maximum contour levels for a variable**"
      ],
      "metadata": {
        "id": "rk3CwXBDoBFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't modify this unless you want to update contour levels\n",
        "\n",
        "# These levels looked okay to me, but could almost certainly\n",
        "# benefit from some adjustments\n",
        "\n",
        "variable_levels = {\n",
        "    # Surface variables\n",
        "    'msl': {'min': 97500, 'max': 102500, 'name': 'mean sea level pressure', 'units': 'Pa'},\n",
        "    'u10': {'min': -25, 'max': 25, 'name': '10m U wind component', 'units': 'm/s'},\n",
        "    'v10': {'min': -25, 'max': 25, 'name': '10m V wind component', 'units': 'm/s'},\n",
        "    'u100': {'min': -30, 'max': 30, 'name': '100m U wind component', 'units': 'm/s'},\n",
        "    'v100': {'min': -30, 'max': 30, 'name': '100m V wind component', 'units': 'm/s'},\n",
        "    't2': {'min': 179, 'max': 321, 'name': '2m temperature', 'units': 'K'},\n",
        "    'tcwv': {'min': 0, 'max': 70, 'name': 'total column water vapor', 'units': 'kg/m²'},\n",
        "    'sp': {'min': 75000, 'max': 105000, 'name': 'surface pressure', 'units': 'Pa'},\n",
        "    'tp': {'min': 0, 'max': 2, 'name': 'total precipitation', 'units': 'm'},\n",
        "\n",
        "    # Geopotential height (z) variables\n",
        "    'z50': {'min': 168000, 'max': 213000, 'name': 'geopotential at 50 hPa', 'units': 'm^2/s^2'},\n",
        "    'z100': {'min': 133000, 'max': 166000, 'name': 'geopotential at 100 hPa', 'units': 'm^2/s^2'},\n",
        "    'z150': {'min': 109000, 'max': 143000, 'name': 'geopotential at 150 hPa', 'units': 'm^2/s^2'},\n",
        "    'z200': {'min': 92000, 'max': 124000, 'name': 'geopotential at 200 hPa', 'units': 'm^2/s^2'},\n",
        "    'z250': {'min': 80000, 'max': 109000, 'name': 'geopotential at 250 hPa', 'units': 'm^2/s^2'},\n",
        "    'z300': {'min': 69300, 'max': 96400, 'name': 'geopotential at 300 hPa', 'units': 'm^2/s^2'},\n",
        "    'z400': {'min': 52600, 'max': 75700, 'name': 'geopotential at 400 hPa', 'units': 'm^2/s^2'},\n",
        "    'z500': {'min': 39200, 'max': 58900, 'name': 'geopotential at 500 hPa', 'units': 'm^2/s^2'},\n",
        "    'z600': {'min': 28400, 'max': 44600, 'name': 'geopotential at 600 hPa', 'units': 'm^2/s^2'},\n",
        "    'z700': {'min': 19200, 'max': 32300, 'name': 'geopotential at 700 hPa', 'units': 'm^2/s^2'},\n",
        "    'z850': {'min': 6640, 'max': 16400, 'name': 'geopotential at 850 hPa', 'units': 'm^2/s^2'},\n",
        "    'z925': {'min': 1600, 'max': 9600, 'name': 'geopotential at 925 hPa', 'units': 'm^2/s^2'},\n",
        "    'z1000': {'min': -4000, 'max': 4000, 'name': 'geopotential at 1000 hPa', 'units': 'm^2/s^2'},\n",
        "\n",
        "    # Temperature (t) variables\n",
        "    't50': {'min': 180, 'max': 240, 'name': 'temperature at 50 hPa', 'units': 'K'},\n",
        "    't100': {'min': 187, 'max': 237, 'name': 'temperature at 100 hPa', 'units': 'K'},\n",
        "    't150': {'min': 190, 'max': 240, 'name': 'temperature at 150 hPa', 'units': 'K'},\n",
        "    't200': {'min': 193, 'max': 243, 'name': 'temperature at 200 hPa', 'units': 'K'},\n",
        "    't250': {'min': 196, 'max': 243, 'name': 'temperature at 250 hPa', 'units': 'K'},\n",
        "    't300': {'min': 196, 'max': 252, 'name': 'temperature at 300 hPa', 'units': 'K'},\n",
        "    't400': {'min': 203, 'max': 266, 'name': 'temperature at 400 hPa', 'units': 'K'},\n",
        "    't500': {'min': 209, 'max': 281, 'name': 'temperature at 500 hPa', 'units': 'K'},\n",
        "    't600': {'min': 211, 'max': 293, 'name': 'temperature at 600 hPa', 'units': 'K'},\n",
        "    't700': {'min': 202, 'max': 302, 'name': 'temperature at 700 hPa', 'units': 'K'},\n",
        "    't850': {'min': 198, 'max': 313, 'name': 'temperature at 850 hPa', 'units': 'K'},\n",
        "    't925': {'min': 201, 'max': 319, 'name': 'temperature at 925 hPa', 'units': 'K'},\n",
        "    't1000': {'min': 204, 'max': 323, 'name': 'temperature at 1000 hPa', 'units': 'K'},\n",
        "\n",
        "    # Wind (u and v) components\n",
        "    'u50':{'min': -70, 'max': 70, 'name': 'U wind component at 50 hPa', 'units': 'm/s'},\n",
        "    'u100':{'min': -70, 'max': 70, 'name': 'U wind component at 100 hPa', 'units': 'm/s'},\n",
        "    'u150':{'min': -70, 'max': 70, 'name': 'U wind component at 150 hPa', 'units': 'm/s'},\n",
        "    'u200':{'min': -70, 'max': 70, 'name': 'U wind component at 200 hPa', 'units': 'm/s'},\n",
        "    'u250':{'min': -70, 'max': 70, 'name': 'U wind component at 250 hPa', 'units': 'm/s'},\n",
        "    'u300':{'min': -70, 'max': 70, 'name': 'U wind component at 300 hPa', 'units': 'm/s'},\n",
        "    'u400':{'min': -50, 'max': 50, 'name': 'U wind component at 400 hPa', 'units': 'm/s'},\n",
        "    'u500':{'min': -50, 'max': 50, 'name': 'U wind component at 500 hPa', 'units': 'm/s'},\n",
        "    'u600':{'min': -40, 'max': 40, 'name': 'U wind component at 600 hPa', 'units': 'm/s'},\n",
        "    'u700':{'min': -40, 'max': 40, 'name': 'U wind component at 700 hPa', 'units': 'm/s'},\n",
        "    'u850':{'min': -30, 'max': 30, 'name': 'U wind component at 850 hPa', 'units': 'm/s'},\n",
        "    'u925':{'min': -30, 'max': 30, 'name': 'U wind component at 925 hPa', 'units': 'm/s'},\n",
        "    'u1000':{'min': -30, 'max': 30, 'name': 'U wind component at 1000 hPa', 'units': 'm/s'},\n",
        "\n",
        "    'v50':{'min': -70, 'max': 70, 'name': 'V wind component at 50 hPa', 'units': 'm/s'},\n",
        "    'v100':{'min': -70, 'max': 70, 'name': 'V wind component at 100 hPa', 'units': 'm/s'},\n",
        "    'v150':{'min': -70, 'max': 70, 'name': 'V wind component at 150 hPa', 'units': 'm/s'},\n",
        "    'v200':{'min': -70, 'max': 70, 'name': 'V wind component at 200 hPa', 'units': 'm/s'},\n",
        "    'v250':{'min': -70, 'max': 70, 'name': 'V wind component at 250 hPa', 'units': 'm/s'},\n",
        "    'v300':{'min': -70, 'max': 70, 'name': 'V wind component at 300 hPa', 'units': 'm/s'},\n",
        "    'v400':{'min': -50, 'max': 50, 'name': 'V wind component at 400 hPa', 'units': 'm/s'},\n",
        "    'v500':{'min': -50, 'max': 50, 'name': 'V wind component at 500 hPa', 'units': 'm/s'},\n",
        "    'v600':{'min': -40, 'max': 40, 'name': 'V wind component at 600 hPa', 'units': 'm/s'},\n",
        "    'v700':{'min': -40, 'max': 40, 'name': 'V wind component at 700 hPa', 'units': 'm/s'},\n",
        "    'v850':{'min': -30, 'max': 30, 'name': 'V wind component at 850 hPa', 'units': 'm/s'},\n",
        "    'v925':{'min': -30, 'max': 30, 'name': 'V wind component at 925 hPa', 'units': 'm/s'},\n",
        "    'v1000':{'min': -30, 'max': 30, 'name': 'V wind component at 1000 hPa', 'units': 'm/s'},\n",
        "\n",
        "    #specific humidity (q)\n",
        "    'q50':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 50 hPa', 'units': 'kg/kg'},\n",
        "    'q100':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 100 hPa', 'units': 'kg/kg'},\n",
        "    'q150':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 150 hPa', 'units': 'kg/kg'},\n",
        "    'q200':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 200 hPa', 'units': 'kg/kg'},\n",
        "    'q250':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 250 hPa', 'units': 'kg/kg'},\n",
        "    'q300':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 300 hPa', 'units': 'kg/kg'},\n",
        "    'q400':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 400 hPa', 'units': 'kg/kg'},\n",
        "    'q500':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 500 hPa', 'units': 'kg/kg'},\n",
        "    'q600':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 600 hPa', 'units': 'kg/kg'},\n",
        "    'q700':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 700 hPa', 'units': 'kg/kg'},\n",
        "    'q850':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 850 hPa', 'units': 'kg/kg'},\n",
        "    'q925':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 925 hPa', 'units': 'kg/kg'},\n",
        "    'q1000':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 1000 hPa', 'units': 'kg/kg'},\n",
        "\n",
        "    # relative Humidity (r)\n",
        "    **{f'r{level}': {'min': 0, 'max': 100, 'name': f'relative humidity at {level} hPa', 'units': '%'}\n",
        "       for level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]},\n",
        "\n",
        "    # Vertical Velocity (w)\n",
        "    **{f'w{level}': {'min': -1, 'max': 1, 'name': f'vertical velocity at {level} hPa', 'units': 'm/s'}\n",
        "       for level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]}\n",
        "}"
      ],
      "metadata": {
        "id": "DqeoqJoWGEtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2) Choose variable information and create animation\n",
        "\n",
        "**You need to fill in some options:**\n",
        "\n",
        "*   **Available variables by model:**\n",
        "\n",
        "<img src=\"https://i.imgur.com/eAHYuPb.png\" alt=\"Variables\" width=\"600\">\n",
        "\n",
        "*   **level (if the variable is on pressure levels):**\n",
        "1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50, or 0 for single level variables\n",
        "*   **lat_bounds and lon_bounds if you want to subset to certain domains**\n",
        "*   **plot_size if you want to make the plot bigger or smaller**\n",
        "*   **colormap if you want something other than viridis**"
      ],
      "metadata": {
        "id": "5PcBjPotlLSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Which variable you want to plot and at what pressure level (if appropriate)\n",
        "#See table for variable options by model\n",
        "variable = \"msl\"\n",
        "level = 0\n",
        "\n",
        "#This bounds the plot (currently set to global)\n",
        "lat_bounds = (5,35)\n",
        "lon_bounds = (-110,-50)\n",
        "\n",
        "#Make the plot bigger or smaller\n",
        "plot_size = 6\n",
        "\n",
        "#Colormap (non-diverging variables)\n",
        "colormap = 'viridis'\n",
        "\n",
        "\n",
        "\n",
        "#Don't need to change anything from here down\n",
        "\n",
        "\n",
        "\n",
        "with xr.open_dataset(f\"{output_path}.nc\") as data_xr:\n",
        "    if level!=0:\n",
        "        varlevel = variable + str(level)\n",
        "        data_sub = data_xr[variable].sel(level=level).load()\n",
        "    else:\n",
        "        varlevel = variable\n",
        "        data_sub = data_xr[variable].load()\n",
        "\n",
        "max_val = variable_levels[varlevel]['max']\n",
        "min_val = variable_levels[varlevel]['min']\n",
        "levels = np.linspace(min_val,max_val,21)\n",
        "variable_name = variable_levels[varlevel]['name']\n",
        "units = variable_levels[varlevel]['units']\n",
        "\n",
        "#These are the variables that need diverging color map\n",
        "if variable in ['u10','v10','u100','v100','u','v','w']:\n",
        "  center = 1\n",
        "  colormap = \"bwr\"\n",
        "else:\n",
        "  center = None\n",
        "  colormap = colormap\n",
        "\n",
        "def convert_longitudes(lon):\n",
        "    lon = np.asarray(lon)\n",
        "    lon = ((lon + 180) % 360) - 180\n",
        "    return lon\n",
        "\n",
        "def plot_data(\n",
        "        data_sub,\n",
        "        plot_size,\n",
        "        lat_bounds,\n",
        "        lon_bounds,\n",
        "        levels,\n",
        "        steps,\n",
        "        colormap,\n",
        "        variable_name,\n",
        "        variable_units,\n",
        "        time_steps=None\n",
        "        ):\n",
        "\n",
        "    # Precompute shifted data for all frames to avoid recalculating in the animation loop\n",
        "    plot_data_shifted_all = [\n",
        "        shiftgrid(\n",
        "            180,\n",
        "            data_sub.isel(time=frame, missing_dims=\"ignore\").values,\n",
        "            data_sub.coords['longitude'].values,\n",
        "            start=False\n",
        "        )[0]\n",
        "        for frame in range(steps)\n",
        "    ]\n",
        "    # Calculate the aspect ratio based on latitude and longitude bounds\n",
        "    lat_extent = lat_bounds[1] - lat_bounds[0]\n",
        "    lon_extent = lon_bounds[1] - lon_bounds[0]\n",
        "    aspect_ratio = lon_extent / lat_extent\n",
        "\n",
        "    # Dynamically adjust the figure size based on the aspect ratio\n",
        "    figure_width = plot_size * aspect_ratio\n",
        "    figure_height = plot_size\n",
        "    figure = plt.figure(figsize=(figure_width, figure_height))\n",
        "\n",
        "    figure.subplots_adjust(left=0, right=1, top=0.90, bottom=0.10, wspace=0, hspace=0)\n",
        "\n",
        "    ax = figure.add_subplot(1, 1, 1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Convert longitudes\n",
        "    lon = data_sub.coords['longitude'].values\n",
        "    lat = data_sub.coords['latitude'].values\n",
        "    plot_data_shifted, lon_shifted = shiftgrid(180, plot_data_shifted_all[0], lon, start=False)\n",
        "\n",
        "    # Create basemap with specified bounds\n",
        "    m = Basemap(projection='cyl', resolution='i', ax=ax,\n",
        "                llcrnrlat=lat_bounds[0], urcrnrlat=lat_bounds[1],\n",
        "                llcrnrlon=lon_bounds[0], urcrnrlon=lon_bounds[1])\n",
        "    m.drawcoastlines()\n",
        "    m.drawcountries()\n",
        "\n",
        "    lon_shifted, lat_shifted = np.meshgrid(lon_shifted, lat)\n",
        "    x, y = m(lon_shifted, lat_shifted)\n",
        "\n",
        "    # Define levels and BoundaryNorm\n",
        "    norm = matplotlib.colors.BoundaryNorm(levels, ncolors=256)\n",
        "\n",
        "    # Initialize the plot with the first frame\n",
        "    im = m.pcolormesh(x, y, plot_data_shifted, norm=norm, cmap=colormap, shading=\"auto\")\n",
        "    cbar = plt.colorbar(\n",
        "        mappable=im,\n",
        "        ax=ax,\n",
        "        orientation=\"horizontal\",\n",
        "        pad=0.02,\n",
        "        aspect=16,\n",
        "        shrink=0.65,\n",
        "        cmap=colormap,\n",
        "        extend=\"neither\")\n",
        "    cbar.set_label(f\"{variable_name} ({variable_units})\")\n",
        "\n",
        "    title = ax.set_title(f\"{variable_name} at Timestep 0\")\n",
        "\n",
        "    # Update function for animation\n",
        "    def update(frame):\n",
        "        im.set_array(plot_data_shifted_all[frame].flatten())\n",
        "        timestep = time_steps[frame] if time_steps is not None else frame\n",
        "        title.set_text(f\"{variable_name} at {timestep} hours\")\n",
        "\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig=figure, func=update, frames=steps, interval=250\n",
        "    )\n",
        "    plt.close(figure.number)\n",
        "    return HTML(ani.to_jshtml())\n",
        "\n",
        "plot_data(data_sub, plot_size, lat_bounds,lon_bounds,\n",
        "          levels,40, colormap, variable_name, units,\n",
        "          time_steps=[i * 6 for i in range(40)])"
      ],
      "metadata": {
        "id": "c2u2TUmwKLAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3) What case is this?\n",
        "\n",
        "If you kept the default date + time + model configuration that I set, you can see that I chose an interesting example! This is FourCastNet-v2's prediction for Hurricane Beryl. At the time of initialization Beryl was a category 5 hurricane.\n",
        "\n",
        "There's actually some unusual behavior here. If you click through the first few time steps, you can see that a low pressure anomaly persists in the original location of the hurricane. This is not a real, physical feature, but rather an artifact arising from the AIWP model. Why? We don't know yet.\n",
        "\n",
        "**I don't want to give the impression that this is common for AIWP models. It is actually quite rare and likely a consequence of the lack of category 5 hurricanes in the training data.** Furthermore, behaviors like this have generally been reduced in newer models like Pangu-Weather and GraphCast. However, it's important to keep in mind that AIWP models don't inherently know anything about physics and that means it's possible for them to produce un-physical or unstable solutions.\n",
        "\n",
        "After you're done with the notebook, try going back and seeing if this behavior persists in Pangu-Weather and GraphCast."
      ],
      "metadata": {
        "id": "Eq5v-0xb5F2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4) Choose the data to plot and plot static images with matplotlib + cartopy\n",
        "\n",
        "**You need to fill in some options again:**\n",
        "\n",
        "*   **Available variables by model:**\n",
        "\n",
        "<img src=\"https://i.imgur.com/eAHYuPb.png\" alt=\"Variables\" width=\"600\">\n",
        "\n",
        "*   **level (if the variable is on pressure levels):**\n",
        "1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50, or 0 for single level variables\n",
        "*   **timestep (hours from initialization; multiple of 6)\n",
        "*   **lat_bounds and lon_bounds if you want to subset to certain domains**\n",
        "*   **colormap if you want something other than viridis**"
      ],
      "metadata": {
        "id": "xrA27GfMEbM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Which variable you want to plot and at what pressure level (if appropriate)\n",
        "#See table for variable options by model\n",
        "variable = \"msl\"\n",
        "level = 0\n",
        "\n",
        "#Time step to plot (hours from initialization; multiple of 6)\n",
        "timestep = 120\n",
        "\n",
        "#This bounds the plot (currently set to global)\n",
        "lat_bounds = (5,35)\n",
        "lon_bounds = (-110,-50)\n",
        "\n",
        "#Colormap (non-diverging variables)\n",
        "colormap = 'viridis'\n",
        "\n",
        "\n",
        "\n",
        "#Don't need to change anything from here down\n",
        "\n",
        "\n",
        "\n",
        "timestepidx = int(timestep/6)\n",
        "\n",
        "with xr.open_dataset(f\"{output_path}.nc\") as data_xr:\n",
        "\n",
        "    if level!=0:\n",
        "        varlevel = variable + str(level)\n",
        "        data_sub = data_xr[variable].sel(level=level)[timestepidx]\n",
        "    else:\n",
        "        varlevel = variable\n",
        "        data_sub = data_xr[variable][timestepidx]\n",
        "\n",
        "max_val = variable_levels[varlevel]['max']\n",
        "min_val = variable_levels[varlevel]['min']\n",
        "levels = np.linspace(min_val,max_val,21)\n",
        "\n",
        "#These are the variables that need diverging color map\n",
        "if variable in ['u10','v10','u100','v100','u','v','w']:\n",
        "  center = 1\n",
        "  colormap = \"bwr\"\n",
        "else:\n",
        "  center = None\n",
        "  colormap = colormap\n",
        "\n",
        "# Create the plot\n",
        "lat_extent = lat_bounds[1] - lat_bounds[0]\n",
        "lon_extent = lon_bounds[1] - lon_bounds[0]\n",
        "aspect_ratio = lon_extent / lat_extent\n",
        "\n",
        "# Dynamically adjust the figure size based on the aspect ratio\n",
        "figure_width = plot_size * aspect_ratio\n",
        "figure_height = plot_size\n",
        "fig = plt.figure(figsize=(figure_width, figure_height))\n",
        "ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "\n",
        "# Set plot bounds\n",
        "ax.set_extent([lon_bounds[0], lon_bounds[1], lat_bounds[0], lat_bounds[1]], crs=ccrs.PlateCarree())\n",
        "\n",
        "# Add country borders and coastlines\n",
        "ax.add_feature(cfeature.BORDERS, linestyle='-', edgecolor='k')\n",
        "ax.add_feature(cfeature.COASTLINE, linewidth=1, edgecolor='k')\n",
        "\n",
        "# Plot the data\n",
        "lon = data_xr['longitude'].values\n",
        "lat = data_xr['latitude'].values\n",
        "contours = ax.contourf(lon, lat, data_sub, levels=levels, cmap=colormap, transform=ccrs.PlateCarree(), extend='both')\n",
        "\n",
        "# Add a color bar\n",
        "cbar = plt.colorbar(contours, ax=ax, orientation='horizontal', pad=0.05,fraction=0.08)\n",
        "cbar.set_label(f\"{variable_levels[varlevel]['name']} ({variable_levels[varlevel]['units']})\")\n",
        "\n",
        "# Add a title\n",
        "plt.title(f\"{variable_levels[varlevel]['name']} at {timestep} hours\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "80mO2k53F9Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice work! If you made it this far you successfully ran an AIWP model from scratch. Feel free to go back and experiment by changing the model, date, time, initial conditions, etc.\n"
      ],
      "metadata": {
        "id": "T9LfTp9HyTEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) AIWP Ensembles\n",
        "\n",
        "AIWP models run so fast that it's quick and easy to produce an ensemble. Whether that ensemble is skillful and calibrated is a different question - prevailing sentiment is that an initial condition AIWP ensemble will be underdispersive.\n",
        "\n",
        "In this section we'll produce a four member ensemble using four sets of perturbed initial conditions from the Global Ensemble Forecast System (GEFS).\n",
        "\n",
        "**Section 4 Estimated Time: 40 minutes**"
      ],
      "metadata": {
        "id": "ZszBsavhyt6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1) Running the ensemble\n",
        "\n",
        "Here is where we run the 4-member ensemble. Relatively speaking it's a bit slow - that's because we're just running the model in sequence four times. You certainly wouldn't want to run a 1000 member ensemble this way! If we had more compute resources we could parallelize the code to run multiple members at once.\n",
        "\n",
        "**There's not too much that's different here from the previous code to run FourCastNet. We just changed the input_data to gefs (from gfs) and placed the ai-models-gfs call in a loop. We also added a --member argument. To summarize, we loop from 0 to 4 and set the member argument based on the loop iteration. Internally, ai-models-gfs gathers the first four GEFS members for us. Basically we are just running ai-models-gfs four different times with four different initial condition perturbations**\n",
        "\n",
        "**Feel free to change the date, time, or model**"
      ],
      "metadata": {
        "id": "wK82sxlA3-nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input parameters\n",
        "\n",
        "# Any date after Jan 1, 2021 in YYYYmmdd format\n",
        "date = \"20240702\"\n",
        "\n",
        "# Initialization time - must be one of [\"0000\", \"0600\", \"1200\", \"1800\"]\n",
        "time = \"1200\"\n",
        "\n",
        "# Model choices:\n",
        "# [\"fourcastnetv2-small\", \"panguweather\", \"graphcast\",\n",
        "# \"graphcast-1p00\", \"aurora-2.5-finetuned\"]\n",
        "model = \"aurora-2.5-finetuned\"\n",
        "\n",
        "# Dataset to use for initial conditions:\n",
        "# \"gfs\" or \"gdas\" are easiest\n",
        "# An empty string defaults to ECMWF MARS (need an account and api key)\n",
        "# \"cds\" uses ERA5 (need an account and api key)\n",
        "input_data = \"gefs\"\n",
        "\n",
        "\n",
        "# How far out to run model in hours\n",
        "forecast_lead = \"240\"\n",
        "\n",
        "\n",
        "# ** Don't change anything beyond this point **\n",
        "\n",
        "# Map model choice to assets path\n",
        "model2assets = {\n",
        "    \"fourcastnetv2-small\":\"./fcnv2\",\n",
        "    \"panguweather\":\"./pw\",\n",
        "    \"aurora-2.5-finetuned\":\"./au\",\n",
        "    \"graphcast\":\"./gc\",\n",
        "    \"graphcast-1p00\":\"./gc\"\n",
        "}\n",
        "\n",
        "assets = model2assets[model]\n",
        "\n",
        "outfiles = []\n",
        "# Runs the ai-models-gfs command\n",
        "for i in range(0,4):\n",
        "    # Path to save output\n",
        "    output_path = f\"./{model}_{date}_{time}_mem{str(i).zfill(2)}\"\n",
        "    outfiles.append(output_path)\n",
        "    !ai-models-gfs \\\n",
        "        --input {input_data} \\\n",
        "        --date {date} \\\n",
        "        --time {time} \\\n",
        "        --member {i} \\\n",
        "        --assets {assets} \\\n",
        "        --path {output_path} \\\n",
        "        --lead {forecast_lead} \\\n",
        "        {model}\n"
      ],
      "metadata": {
        "id": "fAoOmhzEpqTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2) Convert all four files to netCDFs\n",
        "\n",
        "This is the same code as before, just running in a loop to convert all four files.\n",
        "\n",
        "**Run this cell as is.**\n",
        "\n",
        "**Warning: This takes a long time (~25 minutes).**"
      ],
      "metadata": {
        "id": "naGmFeBEiKpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ** Don't modify any of this **\n",
        "\n",
        "# Define the main function to convert GRIB files to NetCDF\n",
        "def grib2nc(model, path, lead, step, date, time, inputmodel):\n",
        "\n",
        "    # Mapping of variable names and their descriptions/units\n",
        "    varmap = {\n",
        "        \"u10\": ['10 metre U wind component', 'm s-1'],\n",
        "        \"v10\": ['10 metre V wind component', 'm s-1'],\n",
        "        \"t2\": ['2 metre temperature', 'K'],\n",
        "        \"msl\": ['Pressure reduced to MSL', 'Pa'],\n",
        "        \"u100\": ['100 metre U wind component', 'm s-1'],\n",
        "        \"v100\": ['100 metre V wind component', 'm s-1'],\n",
        "        \"sp\": ['Surface pressure', 'Pa'],\n",
        "        \"tcwv\": ['Precipitable water', 'kg m-2'],\n",
        "        \"apcp\": ['6-hr accumulated precipitation', 'm'],\n",
        "        \"u\": ['U component of wind', 'm s-1'],\n",
        "        \"v\": ['V component of wind', 'm s-1'],\n",
        "        \"t\": ['Temperature', 'K'],\n",
        "        \"z\": ['Geopotential', 'm2 s-2'],\n",
        "        \"r\": ['Relative humidity', '%'],\n",
        "        \"q\": ['Specific humidity', 'kg kg-1'],\n",
        "        \"w\": ['Vertical velocity', 'Pa s-1']\n",
        "    }\n",
        "\n",
        "    # Mapping from ECMWF to GFS names\n",
        "    ec2gfsmap = {\n",
        "        \"u\": \"u\",\n",
        "        \"v\": \"v\",\n",
        "        \"w\": \"w\",\n",
        "        \"z\": \"z\",\n",
        "        \"q\": \"q\",\n",
        "        \"r\": \"r\",\n",
        "        \"t\": \"t\",\n",
        "        \"10u\": \"u10\",\n",
        "        \"10v\": \"v10\",\n",
        "        \"100u\": \"u100\",\n",
        "        \"100v\": \"v100\",\n",
        "        \"2t\": \"t2\",\n",
        "        \"msl\": \"msl\",\n",
        "        \"sp\": \"sp\",\n",
        "        \"tcwv\": \"tcwv\",\n",
        "        \"tp\": \"apcp\"\n",
        "    }\n",
        "\n",
        "    # Initialize lists to store variables and levels\n",
        "    unique_pl_vars = []\n",
        "    unique_sfc_vars = []\n",
        "    levels = []\n",
        "\n",
        "    # Open the GRIB file and extract grid shape and lat/lon coordinates\n",
        "    with pg.open(path) as grib:\n",
        "        grib = pg.open(path)\n",
        "        y_shape, x_shape = grib[1].values.shape\n",
        "        lats, lons = grib[1].latlons()\n",
        "        lats = lats[:, 0]\n",
        "        lons = lons[0, :]\n",
        "\n",
        "        # Identify pressure level and surface variables\n",
        "        for grb in grib:\n",
        "            if grb.levelType == \"pl\":\n",
        "                if grb.level not in levels:\n",
        "                    levels.append(grb.level)\n",
        "                if grb.shortName not in unique_pl_vars:\n",
        "                    unique_pl_vars.append(grb.shortName)\n",
        "            else:\n",
        "                if grb.shortName not in unique_sfc_vars:\n",
        "                    unique_sfc_vars.append(grb.shortName)\n",
        "        levels.sort(reverse=True)\n",
        "        levelmap = {level: c for c, level in enumerate(levels)}\n",
        "\n",
        "        # Create NetCDF file and define dimensions\n",
        "        f = DS(f\"{path}.nc\", 'w', format='NETCDF4')\n",
        "        f.createDimension('time', lead // step + 1)\n",
        "        f.createDimension('level', len(levels))\n",
        "        f.createDimension('longitude', x_shape)\n",
        "        f.createDimension('latitude', y_shape)\n",
        "\n",
        "        # Generate time values based on initialization date and time\n",
        "        initdt = datetime.datetime.strptime(f\"{date}{time}\", \"%Y%m%d%H00\")\n",
        "        times = np.array([\n",
        "            int((initdt + datetime.timedelta(hours=int(i))).timestamp())\n",
        "            for i in np.arange(0, lead + step, step)\n",
        "        ])\n",
        "\n",
        "        # Create coordinate variables in NetCDF\n",
        "        create_variable_nochunk(f, 'time', ('time',), times, {\n",
        "            'long_name': 'Date and Time', 'units': 'seconds since 1970-1-1',\n",
        "            'calendar': 'standard'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'longitude', ('longitude',), lons, {\n",
        "            'long_name': 'Longitude', 'units': 'degree'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'latitude', ('latitude',), lats, {\n",
        "            'long_name': 'Latitude', 'units': 'degree'\n",
        "        })\n",
        "        create_variable_nochunk(f, 'level', ('level',), np.array(levels), {\n",
        "            'long_name': 'Isobaric surfaces', 'units': 'hPa'\n",
        "        })\n",
        "\n",
        "        # Define variables in NetCDF for both pressure level and surface variables\n",
        "        for variable in unique_pl_vars + unique_sfc_vars:\n",
        "            dims = ('time', 'level', 'latitude', 'longitude') if variable in unique_pl_vars else ('time', 'latitude', 'longitude')\n",
        "            chunksizes = (1, 1, y_shape, x_shape) if 'level' in dims else (1, y_shape, x_shape)\n",
        "            gfsequivalent = ec2gfsmap[variable]\n",
        "            create_variable(\n",
        "                f, gfsequivalent, dims, None, {\n",
        "                    'long_name': varmap[gfsequivalent][0],\n",
        "                    'units': varmap[gfsequivalent][1]\n",
        "                },\n",
        "                chunksizes\n",
        "            )\n",
        "\n",
        "        # Populate NetCDF variables with data from GRIB\n",
        "        grib.seek(0)\n",
        "        for grb in grib:\n",
        "            shortName = grb.shortName\n",
        "            timestep = int(grb.step / step)\n",
        "            level = grb.level\n",
        "            levelType = grb.levelType\n",
        "            if (shortName == 'z' and levelType == 'sfc') or shortName not in ec2gfsmap.keys():\n",
        "                continue\n",
        "            gfsequivalent = ec2gfsmap[shortName]\n",
        "            vals = grb.values\n",
        "\n",
        "            if levelType == 'pl':\n",
        "                levelind = levelmap[level]\n",
        "                f.variables[gfsequivalent][timestep, levelind, :, :] = vals\n",
        "            elif levelType == 'sfc':\n",
        "                f.variables[gfsequivalent][timestep, :, :] = vals\n",
        "\n",
        "        # Add global attributes to the NetCDF file\n",
        "        f.Conventions = 'CF-1.8'\n",
        "        f.version = '3_2025-02-20'\n",
        "        f.model_name = model\n",
        "        f.initialization_model = inputmodel\n",
        "        f.initialization_time = initdt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        f.first_forecast_hour = \"0\"\n",
        "        f.last_forecast_hour = f\"{lead}\"\n",
        "        f.forecast_hour_step = f\"6\"\n",
        "        f.creation_time = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        f.close()\n",
        "\n",
        "# Functions to create variables in the NetCDF file\n",
        "def create_variable(f, name, dimensions, data, attrs, chunksizes):\n",
        "    dtype = 'i4' if name in ['time', 'level'] else 'f4'\n",
        "    var = f.createVariable(name, dtype, dimensions, compression='zlib', complevel=2, chunksizes=chunksizes)\n",
        "    if data is not None:\n",
        "        var[:] = data\n",
        "    for attr_name, attr_value in attrs.items():\n",
        "        var.setncattr(attr_name, attr_value)\n",
        "\n",
        "def create_variable_nochunk(f, name, dimensions, data, attrs):\n",
        "    dtype = 'i4' if name in ['time', 'level'] else 'f4'\n",
        "    var = f.createVariable(name, dtype, dimensions, compression='zlib', complevel=2)\n",
        "    var[:] = data\n",
        "    for attr_name, attr_value in attrs.items():\n",
        "        var.setncattr(attr_name, attr_value)\n",
        "\n",
        "#Call the function\n",
        "outfilesnc = []\n",
        "for outfile in outfiles:\n",
        "    grib2nc(model, outfile, int(forecast_lead), 6, date, time, input_data)\n",
        "    outfilesnc.append(f\"{outfile}.nc\")\n",
        "    !rm {outfile}"
      ],
      "metadata": {
        "id": "qnO7Jkw1iJTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3) Define the variable information\n",
        "\n",
        "This cell is just a large dictionary defining some appropriate bounds for each variable and other general information like name and units.\n",
        "\n",
        "**Only change this dictionary if you want to update minimum and maximum contour levels for a variable**"
      ],
      "metadata": {
        "id": "YrdqOgAX4hjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't modify this unless you want to update contour levels\n",
        "\n",
        "# These levels looked okay to me, but could almost certainly\n",
        "# benefit from some adjustments\n",
        "\n",
        "variable_levels = {\n",
        "    # Surface variables\n",
        "    'msl': {'min': 97500, 'max': 102500, 'name': 'mean sea level pressure', 'units': 'Pa'},\n",
        "    'u10': {'min': -25, 'max': 25, 'name': '10m U wind component', 'units': 'm/s'},\n",
        "    'v10': {'min': -25, 'max': 25, 'name': '10m V wind component', 'units': 'm/s'},\n",
        "    'u100': {'min': -30, 'max': 30, 'name': '100m U wind component', 'units': 'm/s'},\n",
        "    'v100': {'min': -30, 'max': 30, 'name': '100m V wind component', 'units': 'm/s'},\n",
        "    't2': {'min': 179, 'max': 321, 'name': '2m temperature', 'units': 'K'},\n",
        "    'tcwv': {'min': 0, 'max': 70, 'name': 'total column water vapor', 'units': 'kg/m²'},\n",
        "    'sp': {'min': 75000, 'max': 105000, 'name': 'surface pressure', 'units': 'Pa'},\n",
        "    'tp': {'min': 0, 'max': 2, 'name': 'total precipitation', 'units': 'm'},\n",
        "\n",
        "    # Geopotential height (z) variables\n",
        "    'z50': {'min': 168000, 'max': 213000, 'name': 'geopotential at 50 hPa', 'units': 'm^2/s^2'},\n",
        "    'z100': {'min': 133000, 'max': 166000, 'name': 'geopotential at 100 hPa', 'units': 'm^2/s^2'},\n",
        "    'z150': {'min': 109000, 'max': 143000, 'name': 'geopotential at 150 hPa', 'units': 'm^2/s^2'},\n",
        "    'z200': {'min': 92000, 'max': 124000, 'name': 'geopotential at 200 hPa', 'units': 'm^2/s^2'},\n",
        "    'z250': {'min': 80000, 'max': 109000, 'name': 'geopotential at 250 hPa', 'units': 'm^2/s^2'},\n",
        "    'z300': {'min': 69300, 'max': 96400, 'name': 'geopotential at 300 hPa', 'units': 'm^2/s^2'},\n",
        "    'z400': {'min': 52600, 'max': 75700, 'name': 'geopotential at 400 hPa', 'units': 'm^2/s^2'},\n",
        "    'z500': {'min': 39200, 'max': 58900, 'name': 'geopotential at 500 hPa', 'units': 'm^2/s^2'},\n",
        "    'z600': {'min': 28400, 'max': 44600, 'name': 'geopotential at 600 hPa', 'units': 'm^2/s^2'},\n",
        "    'z700': {'min': 19200, 'max': 32300, 'name': 'geopotential at 700 hPa', 'units': 'm^2/s^2'},\n",
        "    'z850': {'min': 6640, 'max': 16400, 'name': 'geopotential at 850 hPa', 'units': 'm^2/s^2'},\n",
        "    'z925': {'min': 1600, 'max': 9600, 'name': 'geopotential at 925 hPa', 'units': 'm^2/s^2'},\n",
        "    'z1000': {'min': -4000, 'max': 4000, 'name': 'geopotential at 1000 hPa', 'units': 'm^2/s^2'},\n",
        "\n",
        "    # Temperature (t) variables\n",
        "    't50': {'min': 180, 'max': 240, 'name': 'temperature at 50 hPa', 'units': 'K'},\n",
        "    't100': {'min': 187, 'max': 237, 'name': 'temperature at 100 hPa', 'units': 'K'},\n",
        "    't150': {'min': 190, 'max': 240, 'name': 'temperature at 150 hPa', 'units': 'K'},\n",
        "    't200': {'min': 193, 'max': 243, 'name': 'temperature at 200 hPa', 'units': 'K'},\n",
        "    't250': {'min': 196, 'max': 243, 'name': 'temperature at 250 hPa', 'units': 'K'},\n",
        "    't300': {'min': 196, 'max': 252, 'name': 'temperature at 300 hPa', 'units': 'K'},\n",
        "    't400': {'min': 203, 'max': 266, 'name': 'temperature at 400 hPa', 'units': 'K'},\n",
        "    't500': {'min': 209, 'max': 281, 'name': 'temperature at 500 hPa', 'units': 'K'},\n",
        "    't600': {'min': 211, 'max': 293, 'name': 'temperature at 600 hPa', 'units': 'K'},\n",
        "    't700': {'min': 202, 'max': 302, 'name': 'temperature at 700 hPa', 'units': 'K'},\n",
        "    't850': {'min': 198, 'max': 313, 'name': 'temperature at 850 hPa', 'units': 'K'},\n",
        "    't925': {'min': 201, 'max': 319, 'name': 'temperature at 925 hPa', 'units': 'K'},\n",
        "    't1000': {'min': 204, 'max': 323, 'name': 'temperature at 1000 hPa', 'units': 'K'},\n",
        "\n",
        "    # Wind (u and v) components\n",
        "    'u50':{'min': -70, 'max': 70, 'name': 'U wind component at 50 hPa', 'units': 'm/s'},\n",
        "    'u100':{'min': -70, 'max': 70, 'name': 'U wind component at 100 hPa', 'units': 'm/s'},\n",
        "    'u150':{'min': -70, 'max': 70, 'name': 'U wind component at 150 hPa', 'units': 'm/s'},\n",
        "    'u200':{'min': -70, 'max': 70, 'name': 'U wind component at 200 hPa', 'units': 'm/s'},\n",
        "    'u250':{'min': -70, 'max': 70, 'name': 'U wind component at 250 hPa', 'units': 'm/s'},\n",
        "    'u300':{'min': -70, 'max': 70, 'name': 'U wind component at 300 hPa', 'units': 'm/s'},\n",
        "    'u400':{'min': -50, 'max': 50, 'name': 'U wind component at 400 hPa', 'units': 'm/s'},\n",
        "    'u500':{'min': -50, 'max': 50, 'name': 'U wind component at 500 hPa', 'units': 'm/s'},\n",
        "    'u600':{'min': -40, 'max': 40, 'name': 'U wind component at 600 hPa', 'units': 'm/s'},\n",
        "    'u700':{'min': -40, 'max': 40, 'name': 'U wind component at 700 hPa', 'units': 'm/s'},\n",
        "    'u850':{'min': -30, 'max': 30, 'name': 'U wind component at 850 hPa', 'units': 'm/s'},\n",
        "    'u925':{'min': -30, 'max': 30, 'name': 'U wind component at 925 hPa', 'units': 'm/s'},\n",
        "    'u1000':{'min': -30, 'max': 30, 'name': 'U wind component at 1000 hPa', 'units': 'm/s'},\n",
        "\n",
        "    'v50':{'min': -70, 'max': 70, 'name': 'V wind component at 50 hPa', 'units': 'm/s'},\n",
        "    'v100':{'min': -70, 'max': 70, 'name': 'V wind component at 100 hPa', 'units': 'm/s'},\n",
        "    'v150':{'min': -70, 'max': 70, 'name': 'V wind component at 150 hPa', 'units': 'm/s'},\n",
        "    'v200':{'min': -70, 'max': 70, 'name': 'V wind component at 200 hPa', 'units': 'm/s'},\n",
        "    'v250':{'min': -70, 'max': 70, 'name': 'V wind component at 250 hPa', 'units': 'm/s'},\n",
        "    'v300':{'min': -70, 'max': 70, 'name': 'V wind component at 300 hPa', 'units': 'm/s'},\n",
        "    'v400':{'min': -50, 'max': 50, 'name': 'V wind component at 400 hPa', 'units': 'm/s'},\n",
        "    'v500':{'min': -50, 'max': 50, 'name': 'V wind component at 500 hPa', 'units': 'm/s'},\n",
        "    'v600':{'min': -40, 'max': 40, 'name': 'V wind component at 600 hPa', 'units': 'm/s'},\n",
        "    'v700':{'min': -40, 'max': 40, 'name': 'V wind component at 700 hPa', 'units': 'm/s'},\n",
        "    'v850':{'min': -30, 'max': 30, 'name': 'V wind component at 850 hPa', 'units': 'm/s'},\n",
        "    'v925':{'min': -30, 'max': 30, 'name': 'V wind component at 925 hPa', 'units': 'm/s'},\n",
        "    'v1000':{'min': -30, 'max': 30, 'name': 'V wind component at 1000 hPa', 'units': 'm/s'},\n",
        "\n",
        "    #specific humidity (q)\n",
        "    'q50':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 50 hPa', 'units': 'kg/kg'},\n",
        "    'q100':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 100 hPa', 'units': 'kg/kg'},\n",
        "    'q150':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 150 hPa', 'units': 'kg/kg'},\n",
        "    'q200':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 200 hPa', 'units': 'kg/kg'},\n",
        "    'q250':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 250 hPa', 'units': 'kg/kg'},\n",
        "    'q300':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 300 hPa', 'units': 'kg/kg'},\n",
        "    'q400':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 400 hPa', 'units': 'kg/kg'},\n",
        "    'q500':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 500 hPa', 'units': 'kg/kg'},\n",
        "    'q600':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 600 hPa', 'units': 'kg/kg'},\n",
        "    'q700':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 700 hPa', 'units': 'kg/kg'},\n",
        "    'q850':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 850 hPa', 'units': 'kg/kg'},\n",
        "    'q925':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 925 hPa', 'units': 'kg/kg'},\n",
        "    'q1000':{'min': 0, 'max': 0.032, 'name': 'specific humidity at 1000 hPa', 'units': 'kg/kg'},\n",
        "\n",
        "    # relative Humidity (r)\n",
        "    **{f'r{level}': {'min': 0, 'max': 100, 'name': f'relative humidity at {level} hPa', 'units': '%'}\n",
        "       for level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]},\n",
        "\n",
        "    # Vertical Velocity (w)\n",
        "    **{f'w{level}': {'min': -1, 'max': 1, 'name': f'vertical velocity at {level} hPa', 'units': 'm/s'}\n",
        "       for level in [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]}\n",
        "}"
      ],
      "metadata": {
        "id": "2gIOW4eVqE3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.4) Choose variable information and create animation\n",
        "\n",
        "**You need to fill in some options:**\n",
        "\n",
        "*   **variables by model:**\n",
        "\n",
        "<img src=\"https://i.imgur.com/eAHYuPb.png\" alt=\"Variables\" width=\"600\">\n",
        "\n",
        "*   **level (if the variable is on pressure levels):**\n",
        "1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50, or 0 for single level variables\n",
        "*   **lat_bounds and lon_bounds if you want to subset to certain domains**\n",
        "*   **plot_size if you want to make the plot bigger or smaller**\n",
        "*   **colormap if you want something other than viridis**"
      ],
      "metadata": {
        "id": "TzlTg8ylqdvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Which variable you want to plot and at what pressure level (if appropriate)\n",
        "#See table for variable options by model\n",
        "variable = \"msl\"\n",
        "level = 0\n",
        "\n",
        "#This bounds the plot (currently set to global)\n",
        "lat_bounds = (5,35)\n",
        "lon_bounds = (-110,-50)\n",
        "\n",
        "#Make the plot bigger or smaller\n",
        "plot_size = 6\n",
        "\n",
        "#Colormap (non-diverging variables)\n",
        "colormap = 'viridis'\n",
        "\n",
        "\n",
        "\n",
        "#Don't need to change anything from here down\n",
        "\n",
        "\n",
        "\n",
        "if level!=0:\n",
        "    varlevel = variable + str(level)\n",
        "else:\n",
        "    varlevel = variable\n",
        "\n",
        "max_val = variable_levels[varlevel]['max']\n",
        "min_val = variable_levels[varlevel]['min']\n",
        "levels = np.linspace(min_val,max_val,21)\n",
        "variable_name = variable_levels[varlevel]['name']\n",
        "units = variable_levels[varlevel]['units']\n",
        "\n",
        "#These are the variables that need diverging color map\n",
        "if variable in ['u10','v10','u100','v100','u','v','w']:\n",
        "    center = 1\n",
        "    colormap = \"bwr\"\n",
        "else:\n",
        "    center = None\n",
        "    colormap = colormap\n",
        "\n",
        "ensdata = []\n",
        "\n",
        "for outfilenc in outfilesnc:\n",
        "    with xr.open_dataset(f\"{outfilenc}\") as data_xr:\n",
        "        if level!=0:\n",
        "            data_sub = data_xr[variable].sel(level=level).load()\n",
        "        else:\n",
        "            data_sub = data_xr[variable].load()\n",
        "        ensdata.append(data_sub)\n",
        "\n",
        "# Function to convert longitudes to the -180 to 180 range\n",
        "def convert_longitudes(lon):\n",
        "    lon = np.asarray(lon)\n",
        "    lon = ((lon + 180) % 360) - 180\n",
        "    return lon\n",
        "\n",
        "def plot_data_ensemble(\n",
        "        ensdata,\n",
        "        plot_size,\n",
        "        lat_bounds,\n",
        "        lon_bounds,\n",
        "        levels,\n",
        "        steps,\n",
        "        colormap,\n",
        "        variable_name,\n",
        "        variable_units,\n",
        "        time_steps\n",
        "    ):\n",
        "\n",
        "    # Precompute shifted data for all frames to avoid recalculating in the animation loop\n",
        "    plot_data_shifted_all = [\n",
        "        [\n",
        "            shiftgrid(\n",
        "                180,\n",
        "                data_sub.isel(time=frame, missing_dims=\"ignore\").values,\n",
        "                data_sub.coords['longitude'].values,\n",
        "                start=False\n",
        "            )[0]\n",
        "            for frame in range(steps)\n",
        "        ]\n",
        "        for data_sub in ensdata\n",
        "    ]\n",
        "\n",
        "    # Calculate the aspect ratio based on latitude and longitude bounds\n",
        "    lat_extent = lat_bounds[1] - lat_bounds[0]\n",
        "    lon_extent = lon_bounds[1] - lon_bounds[0]\n",
        "    aspect_ratio = lon_extent / lat_extent\n",
        "\n",
        "    # Dynamically adjust the figure size based on the aspect ratio\n",
        "    figure_width = plot_size * aspect_ratio\n",
        "    figure_height = plot_size\n",
        "\n",
        "    # Initialize the figure and adjust margins\n",
        "    figure, axes = plt.subplots(2, 2, figsize=(figure_width, figure_height))\n",
        "    figure.subplots_adjust(left=0.01, right=0.99, top=0.95, bottom=0, wspace=-0.15, hspace=0.20)\n",
        "\n",
        "    images = []\n",
        "    titles = []\n",
        "\n",
        "    for ax, data_sub, plot_data_shifted in zip(axes.flat, ensdata, plot_data_shifted_all):\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "        # Convert longitudes and latitudes\n",
        "        lon = data_sub.coords['longitude'].values\n",
        "        lat = data_sub.coords['latitude'].values\n",
        "        plot_data_shifted, lon_shifted = shiftgrid(180, plot_data_shifted[0], lon, start=False)\n",
        "\n",
        "        # Create a Basemap with specified bounds\n",
        "        m = Basemap(projection='cyl', resolution='l', ax=ax,\n",
        "                    llcrnrlat=lat_bounds[0], urcrnrlat=lat_bounds[1],\n",
        "                    llcrnrlon=lon_bounds[0], urcrnrlon=lon_bounds[1])\n",
        "        m.drawcoastlines()\n",
        "        m.drawcountries()\n",
        "\n",
        "        # Create a mesh grid for plotting\n",
        "        lon_shifted, lat_shifted = np.meshgrid(lon_shifted, lat)\n",
        "        x, y = m(lon_shifted, lat_shifted)\n",
        "\n",
        "        # Define color levels and normalization\n",
        "        norm = matplotlib.colors.BoundaryNorm(levels, ncolors=256)\n",
        "\n",
        "        # Initialize the plot with the first frame\n",
        "        im = m.pcolormesh(x, y, plot_data_shifted, norm=norm, cmap=colormap, shading=\"auto\")\n",
        "\n",
        "        # Store references for animation update\n",
        "        images.append(im)\n",
        "\n",
        "        # Add a title to each subplot\n",
        "        title = ax.set_title(f\"{variable_name} at Timestep 0\")\n",
        "        titles.append(title)\n",
        "\n",
        "    # Add a shared colorbar\n",
        "    cbar = figure.colorbar(\n",
        "        images[0],\n",
        "        ax=axes,\n",
        "        orientation=\"horizontal\",\n",
        "        pad=0.02,\n",
        "        aspect=30,\n",
        "        shrink=0.75,\n",
        "        cmap=colormap,\n",
        "        extend=\"neither\"\n",
        "    )\n",
        "    cbar.set_label(f\"{variable_name} ({variable_units})\")\n",
        "\n",
        "    # Update function for animation\n",
        "    def update(frame):\n",
        "        for im, plot_data_shifted, title in zip(images, plot_data_shifted_all, titles):\n",
        "            im.set_array(plot_data_shifted[frame].flatten())\n",
        "            timestep = time_steps[frame] if time_steps is not None else frame\n",
        "            title.set_text(f\"{variable_name} at {timestep} hours\")\n",
        "\n",
        "    # Create the animation\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig=figure, func=update, frames=steps, interval=250\n",
        "    )\n",
        "    plt.close(figure.number)\n",
        "\n",
        "    # Return the HTML animation\n",
        "    return HTML(ani.to_jshtml())\n",
        "\n",
        "\n",
        "plot_data_ensemble(ensdata, plot_size, lat_bounds,lon_bounds,\n",
        "          levels,40, colormap, variable_name, units,\n",
        "          time_steps=[i * 6 for i in range(40)])"
      ],
      "metadata": {
        "id": "ylv1rCWaqcoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "AIWP models are powerful new weather forecasting tools and this notebook shows just how easy it is to use them. This is in contrast to running a NWP mdoel, which is cumbersome and time-consuming.\n",
        "\n",
        "In many regards they match or exceed the performance of traditional NWP models. That said, we are still learning about their strengths, weaknesses, and limitations.\n",
        "\n",
        "Go back and experiment with different models and configurations - if you find anything neat or weird or any bugs I'd like to hear about it (jacob.radford@noaa.gov)!"
      ],
      "metadata": {
        "id": "eShrunrm4-lh"
      }
    }
  ]
}